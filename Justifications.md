## Justification for loss fn used
Both YAMNet and Wav2Vec2 use cross-entropy loss, but with a slight difference in implementation. YAMNet employs sparse categorical cross-entropy (sparse_categorical_crossentropy), while Wav2Vec2 uses cross-entropy loss (torch.nn.CrossEntropyLoss). The key distinction between the two is how they handle labels—sparse categorical cross-entropy is used when labels are provided as integer class indices (e.g., 0 for crying, 1 for screaming, 2 for normal), whereas standard cross-entropy requires labels to be one-hot encoded. Since YAMNet takes integer labels directly, sparse categorical cross-entropy is computationally more efficient as it avoids converting labels into one-hot format. On the other hand, Wav2Vec2, implemented in PyTorch, uses torch.nn.CrossEntropyLoss, which inherently supports integer labels but also works with one-hot encoded labels when needed. Both loss functions are well-suited for multi-class classification, as they minimize the divergence between predicted probabilities and actual labels, ensuring the models learn meaningful patterns from audio data. Given the structured data split (70% training, 15% validation, 15% testing), these loss functions help optimize classification accuracy while preventing overfitting and ensuring generalization across unseen samples.
## Justification for training and testing approach
This structured approach to data splitting—allocating 70% for training, 15% for validation, and 15% for testing—ensures that the models learn effectively from a substantial portion of the data while also preventing overfitting and ensuring reliable performance evaluation. The training set provides enough samples for the models to capture meaningful patterns, while the validation set allows for hyperparameter tuning and early stopping, helping to optimize the model without excessive bias towards the training data. Finally, the test set consists of completely unseen data, ensuring an unbiased assessment of how well the model generalizes to real-world scenarios. This balanced split maintains robustness by preventing overfitting on training data and ensures that performance metrics such as accuracy, confusion matrices, and ROC curves reflect the model’s true classification ability across different audio categories .
## Justification for using averaging probabilities appraoch for ensemble system
In this two-model ensemble system, averaging probabilities was chosen as the fusion method because it provides a balanced combination of predictions from both YAMNet and Wav2Vec2. Majority voting is typically more effective when there are three or more models, as it relies on a consensus among multiple classifiers. With only two models, majority voting would result in frequent ties, making it an impractical choice. By averaging the predicted probabilities from both models, we leverage their individual strengths while ensuring a smooth and continuous representation of confidence levels for each class. This approach enhances the overall robustness of classification while avoiding conflicts that a voting-based method might introduce in a two-model setup.
## Justification for choosing experimental and control datasets
In this study, we define the experimental group as the dataset containing audio samples of infant cries and screams, sourced from the Donateacry corpus and other relevant datasets. These sounds are indicative of distress and require accurate classification. Conversely, the control group consists of non-cry, non-scream audio samples, including normal utterances (speech), music, and indoor environmental sounds. The normal utterances dataset, which includes spoken words and sentences, serves as a crucial control because it represents typical vocal expressions without distress, helping the model differentiate between distress-related and non-distress-related sounds.
