## Justification for loss fn used
Both YAMNet and Wav2Vec2 use cross-entropy loss, but with a slight difference in implementation. YAMNet employs sparse categorical cross-entropy (sparse_categorical_crossentropy), while Wav2Vec2 uses cross-entropy loss (torch.nn.CrossEntropyLoss). The key distinction between the two is how they handle labelsâ€”sparse categorical cross-entropy is used when labels are provided as integer class indices (e.g., 0 for crying, 1 for screaming, 2 for normal), whereas standard cross-entropy requires labels to be one-hot encoded. Since YAMNet takes integer labels directly, sparse categorical cross-entropy is computationally more efficient as it avoids converting labels into one-hot format. On the other hand, Wav2Vec2, implemented in PyTorch, uses torch.nn.CrossEntropyLoss, which inherently supports integer labels but also works with one-hot encoded labels when needed. Both loss functions are well-suited for multi-class classification, as they minimize the divergence between predicted probabilities and actual labels, ensuring the models learn meaningful patterns from audio data. Given the structured data split (70% training, 15% validation, 15% testing), these loss functions help optimize classification accuracy while preventing overfitting and ensuring generalization across unseen samples.
