# Detecting_infant_cries_and_screams
This repositary contains the data and code to implement an Ensemble Model for Detecting Infant Cries, Screams, and Normal Utterances.
The datasets for the crying class is in the folders **infant_cry**, **donateacry_corpus**
The dataset for the screaming class in the folder **Screaming**
The dataset for the normal utterances is in the folder **utterances**

### Experimental and control datasets
In this study, we define the experimental group as the dataset containing audio samples of infant cries and screams, sourced from the Donateacry corpus and other relevant datasets. These sounds are indicative of distress and require accurate classification. Conversely, the control group consists of non-cry, non-scream audio samples, including normal utterances (speech), music, and indoor environmental sounds. The normal utterances dataset, which includes spoken words and sentences, serves as a crucial control because it represents typical vocal expressions without distress, helping the model differentiate between distress-related and non-distress-related sounds.

### Pre_process_audio.py file explanation
This script preprocesses audio data for training an infant cry and scream classification model using YAMNet and Wav2Vec2.It takes the directory you have stored your data in as input, with the datasets and the output directory name and it organizes data into three categories: crying , screaming , and normal . The script loads each audio file, resamples it to a target sampling rate of 16 kHz, and segments longer files into 5-second clips. If a segment is shorter than 5 seconds, it is zero-padded to maintain consistency. To improve model generalization, data augmentation is applied with a 50% probability, either by adding Gaussian noise or applying a random pitch shift. The processed audio segments are then saved in the processed_data directory with a structured naming convention indicating their category and original filename. This ensures a standardized, balanced dataset suitable for training and evaluation.

### Training_Yamnet_and_wave2vec.py file explanation
This script trains two deep learning models, YAMNet and Wav2Vec2, for classifying infant cries, screams, and normal vocalizations. First, it loads the preprocessed .wav files from the processed_data directory, assigns class labels (crying, screaming, and normal), and splits the data into training (70%), validation (15%), and testing (15%) sets using stratified sampling. For YAMNet, the script extracts Mel spectrogram features from the audio files and uses them to train a modified MobileNetV2-based classifier with a softmax output layer. The model is trained using categorical cross-entropy loss and Adam optimizer. For Wav2Vec2, the script processes raw waveform inputs using a pre-trained facebook/wav2vec2-base processor and fine-tunes the model for classification using a cross-entropy loss function and AdamW optimizer. The training loop optimizes the Wav2Vec2 model while periodically evaluating it on the validation set. These two models will later be used individually and in an ensemble for robust audio classification.

### Evaluate_model.py
This script takes the models as input and evaluates the performance of YAMNet, Wav2Vec2, and an ensemble model on the test dataset using classification metrics and visualizations. First, it defines an evaluate_model function that predicts class labels, calculates a confusion matrix, and generates a classification report for a given model. The function is then used to assess the individual performance of YAMNet and Wav2Vec2. Next, an ensemble_predictions function is defined to combine the predictions of both models by averaging their output probabilities. The script also includes a plot_roc_curve function to compute and visualize ROC curves and AUC scores for each class (crying, screaming, and normal). Finally, predictions from YAMNet and Wav2Vec2 are generated, combined using the ensemble method, and evaluated using the same classification metrics. The ensemble approach is expected to leverage the strengths of both models, potentially improving classification accuracy and robustness.
